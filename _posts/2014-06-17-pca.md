---
layout: post
title: PCA
description: ""
modified: 2014-06-17
category: articles
tags: [life, love]---
## 为什么要降维
数据的features之间存在很大的冗余，降维可以减少数据规模，进而减少内存使用和计算规模，加速学习。另一方面，也能够方便可视化features的影响和关联。
![image](/assets/post-images/2014-06-17-9508dae6-36b2-4e2d-aa10-dab2f6b6b5c1.png)
![image](/assets/post-images/2014-06-17-fb2de392-7775-4ac2-c9f4-4158e8fe14ae.png)

## PCA的原理
PCA通过找到一个低纬度空间，然后把每一个样本的features映射从当前的高维度空间映射到这一低维度空间。为了保持数据的特性尽量不改变，低纬度空间R^{n}到每一个样本的距离应可能小。（ps. 可以考虑一种极端情况，二维空间的点都在一条直线上，这样，如果取这条直线的方向作为低维度空间，则距离误差为0。）
![image](/assets/post-images/2014-06-17-8232a53a-ef44-42fc-e205-f7d02ad8db13.png)
![image](/assets/post-images/2014-06-17-ada7273f-5d30-4203-f7ba-7d834b6653f4.png)

## PCA与LR的区别

![image](/assets/post-images/2014-06-17-ba8c312b-c698-4d43-ef15-59cde00e0cec.png)

## 数据预处理
PCA需要做feature scaling。
![image](/assets/post-images/2014-06-17-70f781f1-294f-4599-a04a-53686500f60b.png)

## 算法
先计算协方差矩阵，然后计算特征向量。
![image](/assets/post-images/2014-06-17-9d0cc1a7-345f-4d5d-ead4-d2e03db91c38.png)
![image](/assets/post-images/2014-06-17-b4821e6a-c727-4a10-aab8-1b8139392bb1.png)

## 恢复
利用PCA降维后，也可以反向升回去。但是，由于降维导致了信息损失，升回去的只是近似值。
![image](/assets/post-images/2014-06-17-adf437c1-1dc1-4011-9207-e53ace68ac69.png)
![image](/assets/post-images/2014-06-17-47760a09-998c-4cf9-db43-e220b9961a45.png)


## 主成分个数的选择.
PCA做的是降维，这个过程会导致部分信息的损失。为了使得尽量少的损失信息或者说保证有限的信息损失，我们需要设计一个指标来衡量数据特性的保持程度，也就是% of variance is retained，也就是下图中的百分比。
![image](/assets/post-images/2014-06-17-9e75e2e0-1fee-45c0-a52a-91a568babdff.png)

## PCA的应用

PCA主要应用场景：加速算法或者减少内存使用；需要可视化。
![image](/assets/post-images/2014-06-17-904639e8-f145-4a0e-95de-ff9b06b2209a.png)

PCA不能用来避免overfitting。因为PCA的降维过程会损失信息，使用regularization可以更好达到这一目的。
![image](/assets/post-images/2014-06-17-905137ac-553a-473a-ab73-519700becfc3.png)

一般说来，我们在设计系统/算法的时候，一开始都不会考虑PCA。只有发现程序跑得太慢（我们没有更多计算资源）或者内存太多时才考虑。因此，在设计系统的时候，最开始的计划不应该有PCA。
![image](/assets/post-images/2014-06-17-1e7047e5-56a4-4c51-ca74-a06d5c63e470.png)



