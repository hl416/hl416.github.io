---
layout: post
title: hive
description: ""
modified: 2014-08-14
category: articles
tags: [life, love]
---


I would like to process the data row by rwo, then found UDF might be a good way. 

add file /path/to/python/script.py

then in the sql:

using "script.py" (do not use the full path, otherwise where would be the following error)

Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: [Error 20001]: An error occurred while reading or writing to your custom script. It may have crashed with an error.
	at org.apache.hadoop.hive.ql.exec.ScriptOperator.processOp(ScriptOperator.java:410)
	at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:504)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:847)
	at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:87)
	at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:504)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:847)
	at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:91)
	at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:504)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:847)
	at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:519)
	... 9 more
Caused by: java.io.IOException: Stream closed
	at java.lang.ProcessBuilder$NullOutputStream.write(ProcessBuilder.java:434)
	at java.io.OutputStream.write(OutputStream.java:116)
	at java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)
	at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)
	at java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)
	at java.io.DataOutputStream.write(DataOutputStream.java:107)
	at org.apache.hadoop.hive.ql.exec.TextRecordWriter.write(TextRecordWriter.java:53)
	at org.apache.hadoop.hive.ql.exec.ScriptOperator.processOp(ScriptOperator.java:378)
	... 18 more


but after several days, this error happen again.

I debuged it for a long time, and then found the problem is one argument my python script returned back contains blank or "\t", so that hive cannot interpret the returned data correctly.

This error may mean: 1) there is bug in your python script ; 2) the data input/data output problems with your scripts, usually format..


Using cloudera manager to restart hive server and metastore server, but failed with "150 sec time out". "service cloudera-scm-agent restart" do not help, then found "service cloudera-scm-agent hard_restart" helps.


I updated hive not using cloudera, but directly downloaded the binaries from internet, and then copy the files according to the file locations of hive in cloudera. but then i found that every time i start hive, there would be a derby embeded database and log file in the dir where i run the "hive" command. I searched all the "hive-site.xml" files, bu failed to find any file specifies that hive uses derby. finally sovled by create a new hive-site.xml file with correct conf in the dir where "hive-default.xml.template " located. My newly installed hive uses the conf file there. The path is :/opt/cloudera/parcels/CDH-5.1.0-1.cdh5.1.0.p0.53/lib/hive/conf/hive-site.xml, instead of the already existed file:/opt/cloudera/parcels/CDH-5.1.0-1.cdh5.1.0.p0.53/etc/hive/conf.dist/hive-site.xml.

In hive, we can confirm the parameters/confs that hive uses by lauch hive and then use commands like "set hive.stats.dbconnectionstring". I learned that my hive uses derby using this command.


"Class definition not found", by looking at the shell scripts, i found that hive load some jars, but when i copied the new version hive directly, the links do not working cos the file names including the new version string "0.13.0"
