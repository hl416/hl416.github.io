---
layout: post
title: Spark
description: ""
modified: 2015-02-10
category: articles
tags: [life, love]
---


'sc' is not defined.

'sc' will be created automatically if you start a spark shell. but if you run your own script file, then you need to create it yourself.

from pyspark import SparkContext
sc = SparkContext("local", "App Name", pyFiles=['MyFile.py', 'lib.zip', 'app.egg'])

or 


from pyspark import SparkConf, SparkContext
conf = (SparkConf()
         .setMaster("local")
         .setAppName("My app")
         .set("spark.executor.memory", "1g"))
sc = SparkContext(conf = conf)
https://spark.apache.org/docs/0.9.1/python-programming-guide.html


After I upgraded CHD to 5.3, spark history server fail to start:
Caused by: java.lang.IllegalArgumentException: Log directory specified does not exist: file:/user/spark/applicationHistory.
	at org.apache.spark.deploy.history.FsHistoryProvider.initialize(FsHistoryProvider.scala:101)
	at org.apache.spark.deploy.history.FsHistoryProvider.<init>(FsHistoryProvider.scala:91)
	... 6 more


This is because the log dir point to local file system by default since 5.2.

 http://www.cloudera.com/content/cloudera/en/documentation/core/latest/topics/cdh_rn_spark_ic.html

So, add "hdfs:" to the dir path would fix this problem


Map Reduce用的是BSI model，好处是鲁棒性好，如果一个任务失败了，重跑或者不要就行了。坏处是需要将中间的结果存储起来，也就是要放到磁盘，一方面IO的开销很大，另一方面存储和获取结果分别需要序列化和反序列化，这个计算开销也是很大的。

MPI model的并行性很好，但是如果一个任务失败，整个大任务就需要重新跑。

MPI中的并行是显示的，一个task会通过信号量来与其他的task进行交互。但是在spark中并行是隐式的。

Scala是函数式语言，用消息机制来替代锁，并行性非常好。Spark是用scala写的。

听说点评的hive任务占到了99.5%以上。

HDFS在设计的时候没有考虑memory，因为他们考虑的是PB量级的数据，而当时的memory很小。而传统的database充分用了memory来做buffer和cache。Spark其实是用传统Database的思路来对HDFS做了改造。

Spark以数据为中心：DAG->Optimization->Stage->并行Task，其实和传统数据库很像。

MapReduce认为所有的数据就是字符串，但是在spark的RDD中是包含schema的。

Spark的RDD在transform时是不需要序列化和反序列化的，但是load data和write data -> hdfs的时候当然是需要的。与map-reduce相比，减少了很多序列化相关的工作。

可以理解为，RDD是schema+data的组合，在计算的过程中，schema在不停地变化，对数据的解释在不停地变化。

如果我认为我的某个中间结果经常出错，于是我可以考虑把它放到disk，也就是使用persist。

关系数据库最早是给银行等交易系统设计的，Transation对于这种场景很有用。但是在互联网时代，应用场景对于transaction需求不强烈，其实hadoop这类大数据系统是革了以前使用采样来查询分析的系统的命。

Spark其实特别靠近DBMS。

Spark SQL在语法/接口上是与Hive完全兼容的。

点评没有用spark streaming，而是使用elastic search，但是elsatic search对于distinct计算的时候会出错，于是他们融入了基数排序。

Spark的优势：1）对Python, Scala, Java不同语言的支持；2）充分利用内存，避免磁盘访问；3）不是像mapreduce那种one-pass的计算，而是可以在内存里不断迭代计算；

写了一个简单的spark来filter URL中的中文的脚本，但是发生了奇怪的问题：使用某一天的数据，很正常；但是使用全量数据（一个月左右），则CPU利用率很低。看了各种metrics，查了各种资料，都没有解决。最后发现是因为全量数据是tar格式的，而那一天的则是已经解压出来的。

"15/03/08 11:46:27 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory"这个错误是说配置的memory不够高，需要在UI上的spark configurations里面设置一下java heap就行了。我把worker_max_heapsize和executor_total_max_heapsize分别设置为了64G和128G。

设置spark的log、work、shuffle等的目录时，一定要确保spark用户是可写的。我直接设置为了777。

"bigger than spark.driver.maxResultSize" 设置spark.driver.maxResultSize


"movieRatingsSortedAggregated = movieRatings.map(extractMovieRatingData).reduceByKey(lambda a, b: a+' '+b).filter(lambda line:len(line[1]) > 100 and len(line[1]) < 5000).take(1000)"

在没有take之前，最后的任务非常非常慢，GC时间非常长，貌似无法结束。有了take之后，整个脚本的时长为2 hours。

将filter放在更早的地方，能明显加快速度。算法、流程的设计在哪里都不能忽略。另外，对于本身没有filter的代码，也要考虑一下，是不是能够加一下filter，是不是真的需要所有的数据。

把work node从两个增加到3个，把每个节点的executor memory从140增加到190，发现性能大大提升：尽量用完一切可用的资源。
